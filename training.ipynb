{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.learner import *\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "from utils import *\n",
    "import dill as pickle\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "# Unlike language models (which need a tokenizer to recognize don't as similar to 'do not', \n",
    "# here I have specific encodings for the music, and we can tokenize directly just by splitting by space.\n",
    "def music_tokenizer(x): return x.split(\" \")\n",
    "    \n",
    "def main(model_to_load, training, model_out, test, train, bs, bptt, em_sz, nh, nl, min_freq, dropout_multiplier, epochs):\n",
    "    \"\"\" Loads test/train data, creates a model, trains, and saves it\n",
    "    Input: \n",
    "        model_to_load - if continuing training on previously saved model\n",
    "        model_out - name for saving model\n",
    "        bs - batch size\n",
    "        bptt - back prop through time \n",
    "        em_sz - embedding size\n",
    "        nh - hidden vector size\n",
    "        nl - number of LSTM layers\n",
    "        min_freq - ignore words that don't appear at least min_freq times in the corpus\n",
    "        dropout_multiplier - 1 defaults to AWD-LSTM paper (the multiplier scales all these values up or down)\n",
    "        epochs - number of cycles between saves \n",
    "        \n",
    "    Output:\n",
    "        Trains model, and saves under model_out_light, _med, _full, and _extra\n",
    "        Models are saved at data/models\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    PATHS=create_paths()\n",
    "    \n",
    "    # Check test and train folders have files\n",
    "    train_files=os.listdir(PATHS[\"data\"]/train)\n",
    "    test_files=os.listdir(PATHS[\"data\"]/test)\n",
    "    if len(train_files)<2:\n",
    "        print(f'Not enough files in {PATHS[\"data\"]/train}. First run make_test_train.py')\n",
    "        return\n",
    "    if len(test_files)<2:\n",
    "        print(f'Not enough files in {PATHS[\"data\"]/test}. First run make_test_train.py, or increase test_train_split')\n",
    "        return    \n",
    "        \n",
    "    \n",
    "    TEXT = data.Field(lower=True, tokenize=music_tokenizer)\n",
    "    \n",
    "    # Adam Optimizer with slightly lowered momentum \n",
    "    optimizer_function = partial(optim.Adam, betas=(0.7, 0.99))  \n",
    "    \n",
    "    if model_to_load:\n",
    "        print(\"Loading network\")     \n",
    "        params=pickle.load(open(f'{PATHS[\"generator\"]}/{model_to_load}_params.pkl','rb'))\n",
    "        LOAD_TEXT=pickle.load(open(f'{PATHS[\"generator\"]}/{model_to_load}_text.pkl','rb'))\n",
    "        bptt, em_sz, nh, nl = params[\"bptt\"], params[\"em_sz\"], params[\"nh\"], params[\"nl\"]\n",
    "    \n",
    "    FILES = dict(train=train, validation=test, test=test)    \n",
    "    \n",
    "    # Build a FastAI Language Model Dataset from the training and validation set\n",
    "    # Mark as <unk> any words not used at least min_freq times\n",
    "    md = LanguageModelData.from_text_files(PATHS[\"data\"], TEXT, **FILES, bs=bs, bptt=bptt, min_freq=min_freq)\n",
    "\n",
    "    if model_to_load:\n",
    "        print(TEXT==LOAD_TEXT)\n",
    "        TEXT=LOAD_TEXT\n",
    "        \n",
    "    print(\"\\nCreated language model data.\")\n",
    "    print(\"Vocab size: \"+str(md.nt))\n",
    "        \n",
    "    # AWD LSTM model parameters (with dropout_multiplier=1, these are the values recommended \n",
    "    # by the AWD LSTM paper. For notewise encoding, I found that higher amounts of dropout\n",
    "    # often worked better)\n",
    "    print(\"\\nInitializing model\")\n",
    "    learner = md.get_model(optimizer_function, em_sz, nh, nl, dropouti=0.05*dropout_multiplier, \n",
    "                           dropout=0.05*dropout_multiplier, wdrop=0.1*dropout_multiplier,\n",
    "                           dropoute=0.02*dropout_multiplier, dropouth=0.05*dropout_multiplier)        \n",
    "\n",
    "    # Save parameters so that it's fast to rebuild network in generate.py\n",
    "    dump_param_dict(PATHS, TEXT, md, bs, bptt, em_sz, nh, nl, model_out)\n",
    "    \n",
    "    learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)    # Applying regularization\n",
    "    learner.clip=0.3                                          # Clip the gradients    \n",
    "\n",
    "    if model_to_load:\n",
    "        model_to_load=model_to_load+\"_\"+training+\".pth\"\n",
    "        learner.model.load_state_dict(torch.load(PATHS[\"generator\"]/model_to_load))   \n",
    "        \n",
    "    lrs=[3e-3, 3e-4, 3e-6, 3e-8]\n",
    "    trainings=[\"_light.pth\", \"_med.pth\", \"_full.pth\", \"_extra.pth\"] \n",
    "    save_names=[model_out+b for b in trainings]\n",
    "    save_names=[PATHS[\"generator\"]/s for s in save_names]\n",
    "        \n",
    "    for i in range(len(lrs)):\n",
    "        train_and_save(learner, lrs[i], epochs, save_names[i])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created language model data.\n",
      "Vocab size: 153\n",
      "\n",
      "Initializing model\n",
      "\n",
      "Training models/generator/mod_light.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e769eea85fa5447db7f096c6cd109064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 10610/18770 [16:18<12:53, 10.55it/s, loss=1.07]"
     ]
    }
   ],
   "source": [
    "bs=32\n",
    "bptt=200\n",
    "em_sz=400\n",
    "nh=600\n",
    "nl=4\n",
    "min_freq=1\n",
    "epochs=3\n",
    "prefix=\"mod\"\n",
    "dropout=1\n",
    "training=\"light\"\n",
    "test = \"test\"\n",
    "train =  \"train\"\n",
    "main('', training, prefix, test, train, bs, bptt, em_sz, nh, nl, min_freq, dropout, epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
