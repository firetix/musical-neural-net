{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 216 µs, sys: 76 µs, total: 292 µs\n",
      "Wall time: 296 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "import dill as pickle\n",
    "from fastai.text import * \n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike language models (which need a tokenizer to recognize don't as similar to 'do not', \n",
    "# here I have specific encodings for the music, and we can tokenize directly just by splitting by space.\n",
    "def music_tokenizer(x): return x.split(\" \")\n",
    "    \n",
    "def main(model_to_load, training, model_out, test, train, bs, bptt, em_sz, nh, nl, min_freq, dropout_multiplier, epochs):\n",
    "    \"\"\" Loads test/train data, creates a model, trains, and saves it\n",
    "    Input: \n",
    "        model_to_load - if continuing training on previously saved model\n",
    "        model_out - name for saving model\n",
    "        bs - batch size\n",
    "        bptt - back prop through time \n",
    "        em_sz - embedding size\n",
    "        nh - hidden vector size\n",
    "        nl - number of LSTM layers\n",
    "        min_freq - ignore words that don't appear at least min_freq times in the corpus\n",
    "        dropout_multiplier - 1 defaults to AWD-LSTM paper (the multiplier scales all these values up or down)\n",
    "        epochs - number of cycles between saves \n",
    "        \n",
    "    Output:\n",
    "        Trains model, and saves under model_out_light, _med, _full, and _extra\n",
    "        Models are saved at data/models\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    PATHS=create_paths()\n",
    "    \n",
    "    # Check test and train folders have files\n",
    "    train_files=os.listdir(PATHS[\"data\"]/train)\n",
    "    test_files=os.listdir(PATHS[\"data\"]/test)\n",
    "    if len(train_files)<2:\n",
    "        print(f'Not enough files in {PATHS[\"data\"]/train}. First run make_test_train.py')\n",
    "        return\n",
    "    if len(test_files)<2:\n",
    "        print(f'Not enough files in {PATHS[\"data\"]/test}. First run make_test_train.py, or increase test_train_split')\n",
    "        return    \n",
    "        \n",
    "    \n",
    "    TEXT = data.Field(lower=True, tokenize=music_tokenizer)\n",
    "    \n",
    "    # Adam Optimizer with slightly lowered momentum \n",
    "    optimizer_function = partial(optim.Adam, betas=(0.7, 0.99))  \n",
    "    \n",
    "    if model_to_load:\n",
    "        print(\"Loading network\")     \n",
    "        params=pickle.load(open(f'{PATHS[\"generator\"]}/{model_to_load}_params.pkl','rb'))\n",
    "        LOAD_TEXT=pickle.load(open(f'{PATHS[\"generator\"]}/{model_to_load}_text.pkl','rb'))\n",
    "        bptt, em_sz, nh, nl = params[\"bptt\"], params[\"em_sz\"], params[\"nh\"], params[\"nl\"]\n",
    "    \n",
    "    FILES = dict(train=train, validation=test, test=test)    \n",
    "    \n",
    "    # Build a FastAI Language Model Dataset from the training and validation set\n",
    "    # Mark as <unk> any words not used at least min_freq times\n",
    "    md = LanguageModelData.from_text_files(PATHS[\"data\"], TEXT, **FILES, bs=bs, bptt=bptt, min_freq=min_freq)\n",
    "\n",
    "    if model_to_load:\n",
    "        print(TEXT==LOAD_TEXT)\n",
    "        TEXT=LOAD_TEXT\n",
    "        \n",
    "    print(\"\\nCreated language model data.\")\n",
    "    print(\"Vocab size: \"+str(md.nt))\n",
    "        \n",
    "    # AWD LSTM model parameters (with dropout_multiplier=1, these are the values recommended \n",
    "    # by the AWD LSTM paper. For notewise encoding, I found that higher amounts of dropout\n",
    "    # often worked better)\n",
    "    print(\"\\nInitializing model\")\n",
    "    learner = md.get_model(optimizer_function, em_sz, nh, nl, dropouti=0.05*dropout_multiplier, \n",
    "                           dropout=0.05*dropout_multiplier, wdrop=0.1*dropout_multiplier,\n",
    "                           dropoute=0.02*dropout_multiplier, dropouth=0.05*dropout_multiplier)        \n",
    "\n",
    "    # Save parameters so that it's fast to rebuild network in generate.py\n",
    "    dump_param_dict(PATHS, TEXT, md, bs, bptt, em_sz, nh, nl, model_out)\n",
    "    \n",
    "    learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)    # Applying regularization\n",
    "    learner.clip=0.3                                          # Clip the gradients    \n",
    "\n",
    "    if model_to_load:\n",
    "        model_to_load=model_to_load+\"_\"+training+\".pth\"\n",
    "        learner.model.load_state_dict(torch.load(PATHS[\"generator\"]/model_to_load))   \n",
    "        \n",
    "    lrs=[3e-3, 3e-4, 3e-6, 3e-8]\n",
    "    trainings=[\"_light.pth\", \"_med.pth\", \"_full.pth\", \"_extra.pth\"] \n",
    "    save_names=[model_out+b for b in trainings]\n",
    "    save_names=[PATHS[\"generator\"]/s for s in save_names]\n",
    "        \n",
    "    for i in range(len(lrs)):\n",
    "        train_and_save(learner, lrs[i], epochs, save_names[i])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=32\n",
    "bptt=200\n",
    "em_sz=400\n",
    "nh=600\n",
    "nl=4\n",
    "min_freq=1\n",
    "epochs=3\n",
    "prefix=\"mod\"\n",
    "dropout=1\n",
    "training=\"light\"\n",
    "test = \"test\"\n",
    "train =  \"train\"\n",
    "# main('', training, prefix, test, train, bs, bptt, em_sz, nh, nl, min_freq, dropout, epochs)\n",
    "\n",
    "# PATHS=create_paths()\n",
    "\n",
    "# # Check test and train folders have files\n",
    "# train_files=os.listdir(PATHS[\"data\"]/train)\n",
    "# test_files=os.listdir(PATHS[\"data\"]/test)\n",
    "# if len(train_files)<2:\n",
    "#     print(f'Not enough files in {PATHS[\"data\"]/train}. First run make_test_train.py')\n",
    "# if len(test_files)<2:\n",
    "#     print(f'Not enough files in {PATHS[\"data\"]/test}. First run make_test_train.py, or increase test_train_split')\n",
    "          \n",
    "          \n",
    "\n",
    "\n",
    "# TEXT = data.Field(lower=True, tokenize=music_tokenizer)\n",
    "\n",
    "# Adam Optimizer with slightly lowered momentum \n",
    "# optimizer_function = partial(optim.Adam, betas=(0.7, 0.99))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_folder(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.save('data_lm_export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(PATH, fname='data_lm_export.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.5)\n",
    "# learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2020' class='' max='26815', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      7.53% [2020/26815 01:55<23:32 4.3639]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs=[3e-3, 3e-4, 3e-6, 3e-8]\n",
    "trainings=[\"_light.pth\", \"_med.pth\", \"_full.pth\", \"_extra.pth\"] \n",
    "\n",
    "for i in range(len(lrs)):\n",
    "    learn.fit_one_cycle(1, lrs[i])\n",
    "    learn.save(trainings[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
